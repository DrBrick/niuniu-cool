<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ฅ՞•ﻌ•՞ฅ要抱抱</title>
    <link>https://niub.link/posts/</link>
    <description>Recent content in Posts on ฅ՞•ﻌ•՞ฅ要抱抱</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>牛牛ฅ^•ﻌ•^ฅ</copyright>
    <lastBuildDate>Fri, 09 Oct 2020 19:41:25 +0800</lastBuildDate><atom:link href="https://niub.link/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>RDD知识点梳理</title>
      <link>https://niub.link/posts/about-rdd/</link>
      <pubDate>Fri, 09 Oct 2020 19:41:25 +0800</pubDate>
      
      <guid>https://niub.link/posts/about-rdd/</guid>
      <description>一、RDD是什么有何特点？ 1.1 RDD的基本概念 什么是RDD？Spark官网给出的解释是：
 The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel.
 RDD全称是一个叫弹性分布式数据集的东西，是Spark对数据的核心抽象，也是最关键的抽象，它实质上是一组分布式的JVM不可变对象集合。不可变意味着它是只读的，RDD在经过转换产生新的RDD时，原有的RDD不会改变。
Spark提供一个统一的数据解决方案——RDD，所有需要经过Spark引擎处理的数据都需要被转成RDD，这样便可以提供统一的计算引擎结构。RDD可以处理很多种类型数据，比如JSON、text、集合&amp;hellip;，使用Spark进行分布式计算编程，最核心的就是RDD。
RDD名字中虽然带有分布式，但是它的目的却是要让用户感觉不到分布式，而像操作本地数据集一样操作在分布式存储中的数据。用户可以将RDD看作一个数组，数组并没有保存数据，而是每个数据分区的引用，数据以分区中的元素的形式分散保存在集群中的各个节点上。从这个角度上来说，RDD存储的是元数据而非数据本身。
1.2 RDD中有什么 每个RDD都有如下几个成员：
 分区的集合 用来基于分区进行计算的函数（算子） 依赖（与其他RDD）的集合 对于用来计算出每个分片的地址集合（如HDFS上文件的块存储地址） 对于KV类型的RDD而言有散列分区器  1.3 RDD有什么特性？   RDD维护血统关系图
    延迟计算
    支持持久化
 当对RDD执行持久化操作时，每个节点都会将自己操作的partition数据持久化到内存中，并且之后对该RDD的反复使用中直接使用内存中缓存的partition数据。
巧妙使用RDD持久化，在某些场景下对spark应用程序的性能有很大的提升。
如何持久化一个RDD？调用cache()或者persist()方法即可；cache()是persist()的简化方式，cache()底层调用是persist()无参版本，也就是调用persist(MEMORY_ONLY)，将数据持久化到内存中。
常见的几种持久化策略机制：</description>
    </item>
    
    <item>
      <title>浅析Spark内存管理并附送入门级架构原理</title>
      <link>https://niub.link/posts/spark-mem-and-process/</link>
      <pubDate>Thu, 08 Oct 2020 15:57:05 +0800</pubDate>
      
      <guid>https://niub.link/posts/spark-mem-and-process/</guid>
      <description>一、Spark Executor端内存管理模型详解 1.1 前言 众所周知Spark是一代基于内存进行计算的大数据计算引擎框架，它能够有效的利用内存进行分布式计算。为了更好的利用Spark，深入地理解其内存管理模型有着很重要的意义。当程序出现BUG时能够快速且头脑清醒的定位问题，对工作中Spark调优也有一定的帮助。本文主要探讨的是Spark2.x版本的Executor端的内存模型，Driver端的内存模型在此先不表。
1.2 堆内内存（On-heap Memory） Executor进程作为一个JVM进程，其内存管理建立在JVM的内存管理之上，下图所展示了Executor进程的内存模型：
Executor端堆内内存模型如下图所示：
一般情况下，Spark仅仅使用了堆内内存，Executor端的堆内内存区域可以分为以下四大块：
 Execution内存：主要用于存放Shuffle、Join、Sort、Aggregation等计算过程中的临时数据 Storage内存：主要用于存储Spark的cache数据，例如RDD的cache，Broadcast变量，Unroll数据等。需要注意的是，unrolled的数据如果内存不够，会存储在driver端 用户内存（User Memory）：主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息 预留内存（Reserved Memory）：系统预留内存，用来存储Spark内部对象  1.3 堆外内存（Off-heap Memory） 堆外内存是自Spark1.6之后引入的，这种模式并不在JVM内存中申请内存。所以堆外内存不受JVM内存管理，可以避免频繁的GC，但是有一个缺点是必须用户自己编写内存申请和释放的逻辑。
默认情况下堆外内存是关闭的，可通过spark.memory.offHeap.enabled参数启用，并通过spark.memory.offHeap.size参数设置堆外内存的大小，单位为字节。如果堆外内存被启用，那么Executor内将同时存在堆内和堆外内存，两者的使用互补影响，这个时候Executor中的Execution内存是堆内的Execution内存和堆外的Execution内存之和，同理，Storage内存也一样。
与Executor堆内内存相比，堆外内存只有Execution内存和Storage内存，结构图如下所示：
1.4 Execution内存和Storage内存之间的动态调整 在Spark1.5之前，Execution内存和Storage内存分配是静态的。换言之，如果Execution内存不足，即使Storage内存还有很多空闲内存也是无法被Execution所利用的。
而现在Execution内存和Storage内存可以互相共享（双向共享，单向回收）的。也就是说，如果 Execution内存不足，而Storage内存有空闲，那么Execution可以从 Storage中申请空间；反之亦然。所以上图中的虚线代表Execution内存和Storage内存是可以随着运作动态调整的，这样可以有效地利用内存资源。
Execution内存和Storage内存之间的动态调整图如下所示：
具体的实现逻辑如下：
 程序提交时会设定基本的Execution内存和Storage内存区域（通过 spark.memory.storageFraction 参数设置）； 在程序运行时，如果双方的空间都不足时，则存储到硬盘；将内存中的块存储到磁盘的策略是按照LRU规则进行的。若己方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block） Execution内存的空间被Storage占用后，可让对方将占用的部分转存到硬盘，然后&amp;quot;归还&amp;quot;借用的空间； Storage内存的空间被Execution占用后，目前的实现是无法让对方&amp;quot;归还&amp;quot;，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂；而且 Shuffle 过程产生的文件在后面一定会被使用到，而 Cache 在内存的数据不一定在后面使用。  可见，“双向共享，单向回收”指的是Execution占用的Storage内存无法被Storage所“归还”。
1.5 Task任务如何使用内存 为了更高效的利用宝贵的内存资源，Executor内运行的Task之间共享着 Execution内存。
二、Spark架构原理（入门级） 2.1 Spark计算阶段的划分 Spark会根据应用的复杂程度，将计算程序分割成更多的计算阶段（stage），这些计算阶段组成一个有向无环图DAG，Spark任务调度器可以根据DAG的依赖关系执行计算阶段。
所谓DAG也就是有向无环图，就是说不同阶段的依赖关系是有向的，计算过程只能沿着依赖关系方向执行，被依赖的阶段执行完成之前，依赖的阶段不能开始执行，同时，这个依赖关系不能有环形依赖，否则就成为死循环了。下面这张图展示了一个典型的Spark运行DAG的不同阶段：
从上图可知，整个应用被划分为3个阶段，且阶段3需要依赖阶段1和阶段2，而阶段1和阶段2互不依赖；Spark在执行调度的时候，先执行Stage1和Stage2，再执行Stage3，如果有更多的阶段，其策略也是一样的。
只要根据程序初始化好DAG，然后根据依赖关系顺序执行各个计算阶段。更为详细的说，Spark是通过DAGScheduler负责生成并管理DAG。DAGScheduler根据用户编写的代码生成DAG，然后将程序分发到分布式计算集群，按计算阶段的先后关系调度执行。可见，有了DAG，每个阶段的计算任务划分好了，将程序分发到集群的WorkNode上去执行，便实现了分布式计算。
 ★ 那么问题来了，DAG是根据什么划分Stage的呢？
 答案是根据是否发生Shuffle过程（或者说根据宽窄依赖划分）。
Spark也需要通过shuffle将数据进行重新组合，相同Key的数据放在一起，进行聚合、关联等操作，因而每次shuffle都产生新的计算阶段。这也是为什么计算阶段会有依赖关系，它需要的数据来源于前面一个或多个计算阶段产生的数据，必须等待前面的阶段执行完毕才能进行shuffle，并得到数据。</description>
    </item>
    
    <item>
      <title>HBase建表设计原则和优化</title>
      <link>https://niub.link/posts/hbase-table-and-optimize/</link>
      <pubDate>Wed, 07 Oct 2020 21:39:44 +0800</pubDate>
      
      <guid>https://niub.link/posts/hbase-table-and-optimize/</guid>
      <description>一、关于HBase建表时的几个设计原则 1.1 预分区 背景：HBase默认建表时有一个Region，且这个Region是没有RowKey边界的，所有数据在写入时都是往这个默认的Region中存入数据。随着数据量的不断增加，Region达到一定大小后会自动Split，而Split过程中会产生两个问题：
 数据可能只会往一个Region里面写，造成热点问题； Split是昂贵的操作，会消耗宝贵的集群IO资源；  对策：在创建表的时候，创建多个空Region，并确定每个Region的StartRowKey和EndRowKey，通过RowKey的合理设计使得数据均匀的落在各个Region上，不会存在热点问题；
1.2 RowKey的设计原则 （1）RowKey长度越短越好。HBase中的底层数据是以HFile文件形式组织的，而HFile中是安装KV存储的，如果K过大会极大的影响存储效率；写入数据时会经过MemStore，如果K过长所占空间过大，内存的有效利用就会降低。
（2）RowKey尽量散列。建议将RowKey的高位作为散列字段，使得数据均衡分布在每个RegionServer，提高负载均衡的几率，避免热点问题；
（3）保证RowKey的唯一性
1.3 列族的设计原则 （1）列族名不宜过长
（2）建表时至少指定一个列族，列族一般不超过五个
（3）不同列族的记录数据集不应相差太大
二、关于HBase的常用优化方法 2.1 调优从如下几方面着手 （1）Master高可用：在HBase中Master负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Master宕机则HBase集群将无法使用。
（2）RegionServer预分区：提前大致规划好RegionServer的分区，因为每一个Region维护着StartRowKey和EndRowKey，如果加入的数据RowKey符合某个范围，则把该数据提交给Region。
（3）优化RowKey的设计：RowKey是数据的唯一表示，数据存储在哪个Region取决于RowKey在哪一个预分区区间内，所以合理设计RowKey可在一定程度上防止数据倾斜。
（4）内存优化：HBase操作的过程中需要大量的内存开销，一般会分配整个可用内存的70%给HBase的Java堆，如果配置太大则GC过程持续太久会导致RegionServer长期处于不可用状态。
（5）减少Region分裂次数，给HFile设置合理大小；
2.2 各种减少 （1）关闭Compation操作，合并操作在业务低峰时手动进行；
（2）减少数据量，开启布隆过滤器可提高查询速度，降低网络I/O
（3）数据存储时使用压缩，如Snappy、LZO、GZip等压缩方式
2.3 HMaster和HRgionserver职责 HMaster和HRegionServer分别是Master和RegionServer在操作系统层面上的具体实现，体现的是两个HBase进程；
HMaster的职责：
（1）记录Region在哪台RegionServer服务器上；
（2）新机器加入时负责RegionServer的负载均衡；
（3）Region满足一定大小后需要Split后，HMaster负责Region的分配；
（4）RegionServer宕机之后，负责迁移n多个Region到其它RegionServer；
（5）管理用户对Table表的增删改操作；
HRegionServer的职责：
（1）负责响应用户的IO请求，比如读写HDFS上的数据；
（2）管理多个Region分区；
三、附送：Hive和HBase的区别 3.1 HBase和Hive的区别 共同点：HBase和Hive都是基于Hadoop的，都是使用HDFS作为底层存储。
Hive是基于Hadoop的数据仓库分析工具，其作用是对海量的数据进行离线分析、计算，本质是将Hive SQL转化成MapReduce任务。
Hive不支持索引，所进行的操作都是全表扫描；而HBase支持索引，支持海量数据的随机读操作，一定程度上弥补了HDFS对准实时查询操作的缺陷。
Hive中的表是纯逻辑概念，它本身不存储和计算数据，而HBase中的表是物理表，而且都是以列存储数据的。其内部有一个超大内存的Hash表，用来存储数据的索引，支持随机访问；
HBase负责组织HDFS上的文件（HDFS只负责存储）；</description>
    </item>
    
    <item>
      <title>浅谈HBase入门以及一些学习感悟</title>
      <link>https://niub.link/posts/talk-about-hbase01/</link>
      <pubDate>Tue, 06 Oct 2020 22:34:18 +0800</pubDate>
      
      <guid>https://niub.link/posts/talk-about-hbase01/</guid>
      <description>一、前言 1.1 关于HBase我早就想学了 刚接触大数据的时候就听说过这些名词，Hadoop、HDFS、MapReduce、YARN、Hive、Spark、Flink&amp;hellip;.等，其中也听说过今天的主角——HBase。看到它的简要介绍时不禁发出感叹，好吊啊！能够存储几亿行*几百万列的数据量，当时截图给网友看的时候我都惊呆了。
后面也陆续看了一些入门级别的HBase的教程，脑子里也有一些关于它的印象，什么RegionServer啊，Master啥的，心里也惦记着什么时候来学一学它。
1.2 之前做了些什么？ 相比于其它组件，我还是算比较早接触到HBase的，从安装好了Hadoop集群之后就把HBase分布式数据库也给安装起来了，安装好了之后打开Web监控页面，写个test表建表语句，Web端看看瞧瞧也就算完成任务了。后面就去学习其他大数据组件去了，什么Hive、Spark、Flink、数据仓库、Kafka轮着来，就是没有轮到HBase，于是HBase的学习就一拖再拖&amp;hellip;
1.3 为什么放缓学习HBase的进度？ 偶然的机会进入了一位网课老师的QQ学习内部群，是关于入行大数据的课程的学习群。我在群里有问到他课程体系中怎么没有HBase，他说HBase在这个系列课程中用的不是很多，追求精简就直接把HBase这块的内容给删减掉了。还有一个原因是传统JavaWeb方向也有用到HBase服务的，所以给我的感觉是HBase也重要但是不如其它大数据组件重要，于是就没有把重心放在HBase的学习上，不过我倒是也有在无聊的时候看看它，有时候技术交流群里面也会谈到HBase，也能顺便学到一些知识。
1.4 今天开始学习HBase 昨天晚上躺床上我就想着今天应该把HBase给学了，因为看到很多小伙伴在群里讨论HBase的相关技术，我一句话也插不上，简直白瞎，哈哈( • ̀ω•́ )✧。在慕课网找了两门关于HBase的免费课程，一个偏入门，一个偏存储原理，而今天的这则笔记主要摘自HBase入门的视频课程。
二、HBase入门 2.0 重要的事情才标0 今天在视频中听到老师说了这么一句话“学习任何一门技术之前需要搞清楚意义在哪里，学完之后它能帮助我们解决什么问题，和其它相类似的技术有什么区别。”，这句话很快就被我捕捉到了，我觉得这句话对我帮助很大很大。目前为止也学了蛮多技术了，从前端到后端再到大数据，每个框架每门语言都有着特定的用途，这也是它们在软件开发中的定位。比如Python虽然也能搞Web后端，但是远不如Java搞后端；比如JavaScript天生为Web页面而生，Go语言优秀的性能使得它完美的适合用于处理高并发、区块链的开发之中&amp;hellip;.
于是我又想到了——HBase在Hadoop大数据体系的定义和定位。为什么要考虑这个，换个思路，你是否也要考虑你在这个世界的定义和定位？
2.1 HBase的应用场景及特点  面向列：HBase是面向列的存储，支持独立检索 多版本存储：每一个列的数据存储有多个Version 稀疏性：表中为空的列并不占用存储空间，因此表可以设计的非常稀疏 良好的扩展性：数据底层依赖于HDFS 高可靠性：WAL预写日志机制保证了数据写入时不会因为集群出现异常而导致数据丢失或损坏 高性能：底层的LSM日志合并树和RowKey有序排序的独特架构特性，使得HBase具有非常高的写入性能  2.2 HBase架构体系与设计模型 HBase强烈依赖于两个外部服务：HDFS和Zookeeper。主要包含两个进程：Master、RegionServer（在Linux系统上实际实现的进程分别是：HMaster、HRegionServer；
（本想在这里插入一张HBase体系架构图，但是考虑到Typora插入图片并发布到网上可能会耗费我很多时间，而且今天的时间已经不多了，所以打算明天再来解决插入图片的问题）。
 利用Gitee+PicGo搞了一个免费的Hugo博客图床服务，不得不说Gitee真的很良心，免费提供5G存储空间，大赞！在Typora中设置图床服务为PicGo，在你把图片粘贴到编辑器中时Typora会通过PicGo自动把图片上传到Gitee上，图片链接全网可访问！
 在HBase中有一个Master和多个RegionServer服务，其中RegionServer管理多个Region，Region是HBase实现高性能和负载均衡的基本单位。一个Region含有多个列族，每个列族下有多个列。
关于HBase更多的底层存储细节今天在此不表，打算明天在写一则关于HBase存储原理的博客，进一步强化自己对HBase的理解。比如为什么HBase插入性能很高，为什么随机读的速度很快等常见问题都可以被轻松的回答&amp;hellip;&amp;hellip;(给自己挖坑)！
2.3 HBase数据模型举例说明 关于列族：
 一张表的列族不会超过5个 每个列族中的列数没有限制 列只有插入数据后存在 列在列族中是有序的（字典序）  关于Region：
 每个Region管理若干条数据记录 一个RegionServer中有多个Region，且不同的Region可能会分不到不同的RegionServer上 Region中的数据和其它Region中的数据是互斥的  2.4 HBase表和传统关系型数据表结构的对比 在HBase中，列可以动态增加、数据可自动切分（一个Region达到阈值大小会自动切分成多个Region）、高并发读写，但不支持条件查询。
而传统关系型数据库支持复杂的条件查询，对事务的支持很好，但是不具备HBase所具有的优点。
2.5 HBase的安装部署 安装部署就不赘述了，右手就能做，正经人谁去老老实实的把安装部署的全过程贴出来啊！不过需要值得注意的是安装HBase之前需要做这么几件事：
 JDK1.8以上的安装 Hadoop的安装 Zookeeper的安装 hbase-env.sh的配置 hbase-site.xml的配置 别忘记了，web监控页面端口是16010  2.</description>
    </item>
    
    <item>
      <title>关于Hive和YARNの读书笔记</title>
      <link>https://niub.link/posts/helloyarn/</link>
      <pubDate>Mon, 05 Oct 2020 21:12:17 +0800</pubDate>
      
      <guid>https://niub.link/posts/helloyarn/</guid>
      <description>一、回顾Hive的架构 1.1 Hive的基本结构 在Hadoop 2.x版本以后，Hive所有运行的任务都是交由YARN来统一管理。下图所示为Hive作业的工作流程：
客户端提交SQL作业到HiveServer2, HiveServer2会根据用户提交的SQL作业及数据库中现有的元数据信息（实际生产中存放在MySQL中）生成一份可供计算引擎执行的计划。每个执行计划对应若干MapReduce作业，Hive会将所有的MapReduce作业都一一提交到YARN中，由YARN去负责创建MapReduce作业对应的子任务任务，并协调它们的运行。YARN创建的子任务会与HDFS进行交互，获取计算所需的数据，计算完成后将最终的结果写入HDFS或者本地。
从整个Hive运行作业的过程我们可以知道Hive自身主要包含3个部分：
 第一部分是客户端（Client） 第二部分是HiveServer2 第三部分是元数据以及元数据服务。  1.2 关于Hive的元数据 Hive的元数据保存在Hive的metastore数据中，里面记录着Hive数据库、表、分区和列的一些当前状态信息，通过收集这些状态信息，可以帮助我们更好地监控Hive 数据库当前的状态，提前感知可能存在的问题；可以帮助基于成本代价的SQL 查询优化，做更为正确的自动优化操作。
Hive的元数据主要分为5个大部分：
 数据库相关的元数据 表相关的元数据 分区相关的元数据 文件存储相关的元数据 其他（事务信息、锁信息以及权限相关信息）  1.3 Hive的知识体系 下面一张图摘自《Hive性能调优实战》，全面的展示了Hive的知识体系，其实这篇文章算是我阅读这本书的读书笔记吧，很多知识点讲的比较详细我就直接摘抄下来了。
二、YARN详解 2.1 传统多线程编程和调度框架的对比 在单机程序设计中， 为了快速处理一个大的数据集， 通常采用多线程并行编程。大体流程如下 ： 先由操作系统启动一个主线程， 由它负责数据切分、 任务分配、 子线程启动和销毁等工作， 而各个子线程只负责计算自己的数据， 当所有子线程处理完数据后， 主线程再退出。
而，在生产环境中的大数据集群，所有作业或系统运行所需的资源，都不是直接向操作系统申请，而是交由资源管理器和调度框架代为申请。每个作业或系统所需的资源都是由资源管理和调度框架统一分配、协调。在业界中扮演这一角色的组件有YARN、Mesos等。
2.2 YARN的优点 YARN作为资源调用框架有着如下优点：
 提高系统的资源利用率。不同系统和不同业务在不同的时间点对硬件资源的需求是不一样的，例如一些离线业务通常在凌晨时间启动，除了这个阶段的离线业务对资源的占用比较高，其他时间段基本是空闲的，通过统一资源调度和协调，将这些时间段的资源分配给其他系统。不仅计算资源可以共享，由于允许多套系统部署在一个集群，也能增加系统存储资源的利用率。 协调不同作业/不同系统的资源，减少不同作业和不同系统之间的资源争抢。例如通过资源管理和调度框架并通过一定的资源分配策略，能够保证在多作业情况下，各个作业都能够得到足够的资源得以运行，而不会出现一个作业占用所有资源，导致其他作业全部阻塞的情况。 增强系统扩展性。资源管理和调度框架，允许硬件资源的动态伸缩，而不会影响作业的运行。 资源调度与管理工具把控着资源的分配和任务的调度，直接影响程序的运行效率。如果任务得到资源少了，必将影响自身的程序运行效率，如果任务占用过多资源，在集群任务运行高峰期，必然导致挤占其他任务运行所需的资源。  那么如何利用资源与调度管理工具？作为大数据集群的使用者，基于Hive 做业务的开发者要高效地利用资源与调度管理工具，需要知道两方面的内容：
 YARN运行的基本组成和工作原理，能够基本理清程序运行的整体流程，知道哪些过程或者配置可能成为瓶颈，可以先不用了解，但一定要有意识。 YARN资源调度与分配算法。   这里有个感触就是，对学习任何一个大数据组件是不是也应该采用这种方式呢？第一要熟悉该组件的基本组成和工作原理，对其大体的工作流程要熟悉。当然了看一遍是不可能全部记住以及全部消化和理解的。不如多看几遍，很多陌生的名词头一次看或许印象不深，但是如果多看几遍甚至发到技术群讨论一下，印象和理解或许更加深刻，也有助于对该技术组件的学习。
由此引申HDFS、MapReduce、Kafka、HBase、Spark、Flink&amp;hellip;都按照这种套路来学，会不会有触类旁通的效果呢？
 2.3 YARN的基本组成 YARN的基本结构由一个ResourceManager与多个NodeManager组成。ResourceManager负责对NodeManager所持有的资源进行统一管理和调度。当在处理一个作业时ResourceManager会在NodeManager所在节点创建一全权负责单个作业运行和监控的程序ApplicationMaster。
  ResouceManager（简称RM）
 资源管理器负责整个集群资源的调度，该组件由两部分构成：调度器（Scheduler）和ApplicationsMaster（简称ASM）。调度器会根据特定调度器实现调度算法，结合作业所在的队列资源容量，将资源按调度算法分配给每个任务。分配的资源将用容器（container）形式提供，容器是一个相对封闭独立的环境，已经将CPU、内存及任务运行所需环境条件封装在一起。通过容器可以很好地限定每个任务使用的资源量。YARN调度器目前在生产环境中被用得较多的有两种：能力调度器（CapacityScheduler）和公平调度器（Fair Scheduler）。（还有一种FIFO Scheduler）</description>
    </item>
    
    <item>
      <title>再来一道SQL题</title>
      <link>https://niub.link/posts/keep-water-for-hive/</link>
      <pubDate>Sun, 04 Oct 2020 21:37:37 +0800</pubDate>
      
      <guid>https://niub.link/posts/keep-water-for-hive/</guid>
      <description>一、求每一年最高气温的那一天以及温度 1.1 数据准备 2014010114 2014010216 2014010317 2014010410 2014010506 2012010609 2012010732 2012010812 2012010919 2012011023 2001010116 2001010212 2001010310 2001010411 2001010529 2013010619 2013010722 2013010812 2013010929 2013011023 2008010105 2008010216 2008010337 2008010414 2008010516 2007010619 2007010712 2007010812 2007010999 2007011023 2010010114 2010010216 2010010317 2010010410 2010010506 2015010649 2015010722 2015010812 2015010999 2015011023 数据的含义是：2010012325表示在2010年01月23日的气温为25度。
建表、导入数据：
create table tempertrue(data string); load data local inpath &amp;#34;/data/soft/hive-3.2.1/mydata/tempertrue.txt&amp;#34; into table tempertrue; ※ 1.2 求每一年最高气温的那一天以及温度 首先，先创建一个临时视图，把data分割日期和温度两个字段。
create view temp as select substr(data, 1, 8) as ttime, substr(data, 9) as tep from tempertrue; 利用开窗函数、substr函数对data分割后分组并排序：</description>
    </item>
    
    <item>
      <title>请为Hive开扇窗</title>
      <link>https://niub.link/posts/hive-window-func/</link>
      <pubDate>Sat, 03 Oct 2020 21:01:25 +0800</pubDate>
      
      <guid>https://niub.link/posts/hive-window-func/</guid>
      <description>一、Hive中的开窗函数 1.1 关于开窗函数 什么是开窗函数？
回答这个问题之前先要了解一下什么是聚合函数。聚合函数是SQL的基本函数，其功能是对一组值执行计算，并返回单个值。再说一遍，聚合函数是SQL的基本函数，其功能是对一组值执行计算，并返回单个值。
常见的聚合函数有：sum()，count()，max()，min()， avg()等，这些聚合函数通常与group by 子句连用，除了count以外，聚合函数忽略空值；
上面有说到，聚合函数只返回单个值，而开窗函数则可为窗口中的每行都返回一个值。更简单的理解就是，对查询的结果多出一列或者多列，列中可以是聚合值也可以试排序值。
一般地，开窗函数也叫分析函数，有两类：聚合开窗函数、排序开窗函数。
另外：
 SQL标准运行将所有的聚合函数用作开窗函数，用OVER关键字区分开窗函数和聚合函数。 聚合函数每组只返回一个值，开窗函数每组（每个窗口）可返回多个值。（上面也有说到过为每行都返回一个值）  1.2 举个不太熟栗子 [root@hadoop200m mydata]# more student_score.data 1 zs1 chinese 80 2 zs1 math 90 3 zs1 english 89 4 zs2 chinese 60 5 zs2 math 75 6 zs2 english 80 7 zs3 chinese 79 8 zs3 math 83 9 zs3 english 72 10 zs4 chinese 90 11 zs4 math 76 12 zs4 english 80 13 zs5 chinese 98 14 zs5 math 80 15 zs5 english 70 select * from student_score; +-------------------+---------------------+--------------------+----------------------+ | student_score.</description>
    </item>
    
    <item>
      <title>浅谈SQL语句的执行顺序</title>
      <link>https://niub.link/posts/sql-execution-sequence/</link>
      <pubDate>Fri, 02 Oct 2020 19:10:59 +0800</pubDate>
      
      <guid>https://niub.link/posts/sql-execution-sequence/</guid>
      <description>浅谈SQL语句的执行顺序 一、SQL语句执行顺序解析 SQL语句和编程语言的语句有很大的不同，有一点原因在于SQL并不一定是按照你写的SQL语句顺序执行的。因此在写SQL语句的时候很容易出现问题，最主要的问题还是记不住SQL命令，死记硬背效果好像也不是很好。
需要知道的是，在SQL语句执行的过程中，都会产生一个虚拟表，用来保存SQL语句的执行结果。这些虚拟表对用户来说是透明的，只有最后一步生成的虚拟表才会返回给用户。如果没有再查询中指定某一子句，则将跳过相应的步骤。下面先给出SQL语句的执行顺序，并加以简单的解释。
7) select 8) distinct &amp;lt;select_list&amp;gt; 1) from &amp;lt;left_table&amp;gt; 3) &amp;lt;join_type&amp;gt; join &amp;lt;right_table&amp;gt; 2) on &amp;lt;join_condition&amp;gt; 4) where &amp;lt;where_condition&amp;gt; 5) group by &amp;lt;group_by_list&amp;gt; 6) having &amp;lt;having_condition&amp;gt; 9) order by &amp;lt;order_by_list&amp;gt; 10) limit &amp;lt;limit_number&amp;gt; 1）from：对FROM子句中的左表&amp;lt;left_table&amp;gt;和右表&amp;lt;right_table&amp;gt;执行笛卡儿积（Cartesian product），产生虚拟表VT1。
2）on： 对虚拟表VT1应用ON筛选，只有那些符合&amp;lt;join_condition&amp;gt;的行才被插入虚拟表VT2中。
3）join：如果指定了OUTER JOIN（如LEFT OUTER JOIN、RIGHT OUTERJOIN），那么保留表中未匹配的行作为外部行添加到虚拟表VT2中，产生虚拟表VT3。如果FROM子句包含两个以上表，则对上一个连接生成的结果表VT3和下一个表重复执行步骤1）～步骤3），直到处理完所有的表为止。
 这里出现了两个陌生的名词：“保留表”、“外部行”。
LEFT OUTER JOIN把左表记为保留表，RIGHT OUTER JOIN把右表记为保留表&amp;hellip;
一时半会也解释不清这个东西，我自己懂了就好了，先挖个坑，日后再补上具体数据来做详细解释说明。
 4）where：对虚拟表VT3应用WHERE过滤条件，只有符合&amp;lt;where_condition&amp;gt;的记录才被插入虚拟表VT4中。
5）group by：根据GROUP BY子句中的列，对VT4中的记录进行分组操作，产生VT5。
6）having：对虚拟表VT6应用HAVING过滤器，只有符合&amp;lt;having_condition&amp;gt;的记录才被插入虚拟表VT6中。
7）select：选择指定的列，插入到虚拟表V7中。
 看到没有，select到第7步才被执行，所以千万别人认为写在第一行的就是第一个被执行的。
 8）distinct：去除重复数据，产生虚拟表V8。
9）order by：将虚拟表V8中的记录按照&amp;lt;order_by_list&amp;gt;进行排序操作，产生虚拟表V9。
 ORDER BY 默认是升序排序，如果要指定为降序排序则需要加上 DESC 修饰。</description>
    </item>
    
    <item>
      <title>Hive认识我的第一天</title>
      <link>https://niub.link/posts/hive-meeting/</link>
      <pubDate>Thu, 01 Oct 2020 17:08:58 +0800</pubDate>
      
      <guid>https://niub.link/posts/hive-meeting/</guid>
      <description>一、Hive简介 1.1 什么是Hive Hive官网给出的定义是：“The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.”
我粗糙的理解是，Hive是一个类似Hadoop（集群）的客户端，所谓客户端在手天下我有。就好比你拿到了电视的遥控器，想怎么操作电视就怎么操作电视，比如换台。实际上Hive使用的是类似SQL的语句去操作（离线数据分析、数据管理&amp;hellip;）HDFS上的数据，本质是将SQL语句转化成MapReduce作业。
1.2 为什么要使用Hive Hive的本质是将SQL转化成MapReduce作业，那么直接使用MapReduce不就好了，何必转一个弯呢？因为Hive更方便啊，它是遥控器！MapReduce编程模型还是有一定门槛的，代码量多而且只有map和reduce过程，实现复杂的计算逻辑则难度相当之大。
而，Hive能够使得不会编程的数据开发人员也具备操作分析HDFS上数据的能力，只要你SQL写的好、写的妙。
1.3 Hive有什么特点 先说缺点，这样记得更牢固。
首先，慢，而且不是一般的慢。当然这是对少量数据而言，比如查询十多行的数据需要100秒左右的时间，这哪能忍。
其次，Hive不太支持记录级别的增删改操作。注意是不太支持，你要是真想用还是可以用的，只是要选择对应的Hive版本。
最后，Hive不支持事务。它主要是用来做OLAP（联机分析处理）的，而传统数据库是用来做OLTP（联机事务处理）的，这也是它俩的本质区别之一。
优点，我是记不住，正经人谁记某个东西的优点啊。
首先，延展性好。传统SQL能做的它也能做，传统SQL不能做的它还是能做。比如Hive支持自定义函数UDF，开发人员可以根据自己的需求来实现自定义函数。
其次，拥有HDFS的优点。HDFS的优点也算是它的优点，比如横向扩展。比如加机器的时候是不会对Hive造成影响的，连重启服务也不需要。
最后，良好的容错性。这一点是硬凑的，我理解不到这一层。
稍微总结一下：Hive只适合用来做海量数据离线统计分析，也是数据仓库实现常用的技术之一。
二、Hive的架构  用户接口：包括CLI、JDBC等方式（也就是客户端） 元数据MetaStore：Hive通常将元数据信息保存在传统关系型数据库中，如MySQL，但默认是放在Derby数据库中（这种数据库不支持多用户同时访问）。元数据信息包括：库名、表名、表所属的库名、表的列名/分区字段，表的属性（是否为外部表）、表中数据所在的HDFS目录等信息。【由此可见，元数据的重要性不言而喻。】 Driver：包括：解释器、编译器、优化器、执行器。Hive SQL语句从词法分析、语法分析、编译、优化以及查询计划生成，查询计划存储在HDFS上，MapReduce负责调用执行；  解释器：将用户输入的Hive SQL转化成抽象语法树（AST） 编译器：将AST编译成逻辑执行计划 优化器：将逻辑执行计划进行优化 执行器：优化后的逻辑执行计划转换成可以运行的物理执行计划    三、Hive中数据组织方式 3.</description>
    </item>
    
    <item>
      <title>时隔五个月的少年又回来了</title>
      <link>https://niub.link/posts/%E6%97%B6%E9%9A%94%E4%BA%94%E4%B8%AA%E6%9C%88%E7%9A%84%E5%B0%91%E5%B9%B4%E5%8F%88%E5%9B%9E%E6%9D%A5%E4%BA%86/</link>
      <pubDate>Tue, 29 Sep 2020 23:44:55 +0800</pubDate>
      
      <guid>https://niub.link/posts/%E6%97%B6%E9%9A%94%E4%BA%94%E4%B8%AA%E6%9C%88%E7%9A%84%E5%B0%91%E5%B9%B4%E5%8F%88%E5%9B%9E%E6%9D%A5%E4%BA%86/</guid>
      <description>一眨眼就是五个月，整整五个月，你知道我这五个月都是怎么过来的吗？
迷茫、踌躇、失意、自闭、堕落、颓废、自暴自弃&amp;hellip;
这注定是一段挥之不去的记忆。
#include&amp;lt;stdio.h&amp;gt;int main() { for (int i = 0; i &amp;lt; 10; i++) { printf(&amp;#34;Hello, World!&amp;#34;); } return 0; } 为什么要使用图床 在hugo中，图片是以/static为基准目录的，例如，设baseUrl为https://focksor.gitee.io/，图片文件存放位置是static/img/gitee/123.jpg，那么，编译完成后页面图片引用的地址就是https://focksor.gitee.io/img/gitee/123.jpg，则在Markdown要引用此图片应该用![图片说明](/img/gitee/123.jpg)，但是这样带来的问题是，在写作的时候无法看到图片，要经过hugo编译之后才能看到文档图片，这样的写作显然是不友好的。
图床指存储图片的服务器，使用图床存储文档中的图片，那么在使用图片的时候只要写上图片所在的网络地址就好了，这样比较使用静态图片体验显然要好很多。下面介绍Gitee+PicGo的方法来使用gitee作为图床。
 图床指存储图片的服务器，使用图床存储文档中的图片，那么在使用图片的时候只要写上图片所在的网络地址就好了，这样比较使用静态图片体验显然要好很多。下面介绍Gitee+PicGo的方法来使用gitee作为图床。
一眨眼就是五个月，整整五个月，你知道我这五个月都是怎么过来的吗？
 </description>
    </item>
    
  </channel>
</rss>
